{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: \n",
    "Write a pseudo-code for how you would implement this with a set of nested\n",
    "for loops. The convolution is defined by a set of weights/parameters which we will learn.\n",
    "How do you represent these weights?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in batch_size:\n",
    "#     padding_width = input_width + 2*padding\n",
    "#     padding_height = input_height + 2*padding\n",
    "#     # Given that output_width and output_height are the same:\n",
    "#     output_width = output_height = ((input_width - kernel_size + 2*padding)/stride) + 1\n",
    "#     # Create empty output for this layer\n",
    "#     output = zeros([total_kernels, output_width, output_height])\n",
    "#     for layer in input_channels:\n",
    "#         layer_pad = [sample, layer, :, :]\n",
    "#         # Add padding\n",
    "#         for pad in padding:\n",
    "#             add column of zeros to right to layer_pad\n",
    "#             add column of zeros to left to layer_pad\n",
    "#             add column of zeros to top to layer_pad\n",
    "#             add column of zeros to bottom to layer_pad\n",
    "#         # Move over layer:\n",
    "#         for y in range(output_height-1):\n",
    "#             for x in range(output_width-1):\n",
    "#                 # If multiple kernels exist, save in different slices of output\n",
    "#                 for kernel in total_kernels:\n",
    "#                     patch = layer_pad[y*stride : (y*stride)+kernel_size, x*stride : (x*stride)+kernel_size]\n",
    "#                     output[kernel, y, x] += sum( patch * kernel[layer, :, :] )\n",
    "#     sample_output[sample, :, :, :] = output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. \n",
    "For a given input tensor, kernel size, stride and padding (no dilutions) work out\n",
    "a general function that computes the size of the output.\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html?highlight=functional%20conv2d#torch.nn.functional.conv2d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6.4.1'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import ipykernel\n",
    "import math\n",
    "ipykernel.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n",
      "torch.Size([1, 1, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "tensor = np.random.rand(1, 2, 27, 27)\n",
    "weight = np.random.rand(1, 2, 3, 3)\n",
    "\n",
    "def output_size(input_tensor, kernel_size, stride, padding):\n",
    "    batch_size, channels, height, width = input_tensor.shape # <- similar to tensor.size() in torch, but '.shape' here since it is numpy\n",
    "    out_size = (((height - kernel_size + 2*padding) / stride ) + 1)\n",
    "    return out_size\n",
    "\n",
    "print(output_size(tensor, 3, 2, 0)) #<- example from the slides: Lecture 3 AlexNet, top right\n",
    "\n",
    "tensor = torch.rand([1, 2, 27, 27])\n",
    "weight = torch.rand([1, 2, 3, 3])\n",
    "\n",
    "print(torch.nn.functional.conv2d(tensor, weight, stride=2,padding=0).size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: \n",
    "Write a naive (non-vectorized) implementation of the unfold function in\n",
    "pseudocode. Include the pseudocode in your report.\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.unfold.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #input_tensor = [b, c, h, w] #batch_size, channels, height, width\n",
    "\n",
    "# # Pseudo code naive unfold:\n",
    "# def naive_unfold(input_tensor, kernel_size, stride, padding):\n",
    "#     output_size = output_size(input_tensor, kernel_size, stride, padding)\n",
    "#     #1 extract all patches from the input\n",
    "#     for sample in b:\n",
    "#         for channel in c:\n",
    "#             layer_pad = [sample, layer, :, :]\n",
    "#             # Add padding\n",
    "#             for pad in padding:\n",
    "#                 add column of zeros to right to layer_pad\n",
    "#                 add column of zeros to left to layer_pad\n",
    "#                 add column of zeros to top to layer_pad\n",
    "#                 add column of zeros to bottom to layer_pad\n",
    "#             n_patches_per_layer = output_size * output_size\n",
    "#             for y in range(output_size-1):\n",
    "#                 for x in range(output_size-1):\n",
    "#                     # x+ y = number of patch, total_patch is 0 first time\n",
    "#                     total_patch = (x+y) * channel\n",
    "#                     patch[sample, x+y + total_patch ,:, :] = layer_pad[y*stride : (y*stride)+kernel_size, \n",
    "#                                                                         x*stride : (x*stride)+kernel_size]\n",
    "#     # 2. Flatten these patches (with all channels) into vectors, arranged as the columns of a matrix X.\n",
    "#     #### THIS CORRECT???\n",
    "#     X = patch[:, :].flatten()\n",
    "#     p = len(X)\n",
    "#     # 3. Multiply this matrix by a weight matrix Y = XW\n",
    "#     Y = X * W\n",
    "#     # 4. Reshape the matrix Y, so that its columns become the pixels of the output tensor.\n",
    "#     k = c * output_size * output_size\n",
    "#     Y = Y.reshape([b, k, p])\n",
    "#     return patch\n",
    "#     #output = [b, k, p] #batch_size, number of values per patch, number of patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch Module : niet af!\n",
    "Inspiratie (kreeg ik doorgestuurd): \n",
    "https://discuss.pytorch.org/t/decompose-conv2d-input-unfold-gemm-fold/93740 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batch =  torch.Size([16, 3, 32, 32])\n",
      "unfolded =  torch.Size([16, 27, 1024])\n",
      "X_reshaped =  torch.Size([16384, 27])\n",
      "W =  torch.Size([27, 8])\n",
      "Y =  torch.Size([16384, 8])\n",
      "Y_reshaped =  torch.Size([16, 1024, 8])\n",
      "Y_permuted =  torch.Size([16, 8, 1024])\n",
      "output =  torch.Size([16, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels, kernel_size=(3,3), stride=1, padding=1):\n",
    "        super().__init__() # <- belangrijk!\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        batch_size, channels, height, width = input_batch.size()\n",
    "        print(\"input_batch = \", input_batch.size())\n",
    "        \n",
    "        # output dimensions\n",
    "        h_out = int(output_size(input_batch, self.kernel_size[0], self.stride, self.padding))\n",
    "        w_out = int(output_size(input_batch, self.kernel_size[1], self.stride, self.padding))\n",
    "        \n",
    "        # unfolded matrix (b, k, p)\n",
    "        unfolded = F.unfold(input_batch, self.kernel_size, padding=self.padding, stride=self.stride)\n",
    "        print(\"unfolded = \", unfolded.size())\n",
    "        batch_size, k_values_per_patch, patches = unfolded.size()\n",
    "        \n",
    "        # reshape to (b, p, k) tensor, than merge b and p to get (b*p,k) tensor\n",
    "        reshaped = torch.transpose(unfolded, 1, 2).reshape(-1, k_values_per_patch)\n",
    "        print(\"X_reshaped = \", reshaped.size())\n",
    "        \n",
    "        # Initiate random weights with correct dimensions\n",
    "        W = torch.rand((k_values_per_patch, self.out_channels)) # - rows: number of nodes in one patch of input. -columns: # of nodes in one pixel in output\n",
    "        print(\"W = \", W.size())\n",
    "        \n",
    "        # Matrix multiplication to get Y\n",
    "        Y = torch.mm(reshaped, W) # bmm?\n",
    "        print(\"Y = \", Y.size())\n",
    "        \n",
    "        # Reshape to get seperate batches back\n",
    "        Y_reshaped = Y.reshape((batch_size, patches, self.out_channels)) # contains one row-vector for each pixel in output\n",
    "        print(\"Y_reshaped = \", Y_reshaped.size())\n",
    "        \n",
    "        # Permute to swap axis for p and k\n",
    "        Y_permuted = torch.permute(Y_reshaped, (0, 2, 1))\n",
    "        print(\"Y_permuted = \", Y_permuted.size())\n",
    "        \n",
    "        # Fold back to obtain the output of this layer\n",
    "        output = Y_permuted.reshape(batch_size, self.out_channels, h_out, w_out)\n",
    "        print(\"output = \", output.size())\n",
    "        \n",
    "        assert output.size() == torch.nn.functional.conv2d(input_batch, W.reshape(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1]),padding=self.padding).size()\n",
    "        return output\n",
    "\n",
    "# We use the Conv2D module by instantiating it, and applying it to an input.\n",
    "torch.manual_seed(0)\n",
    "conv = Conv2D(in_channels= 3, out_channels= 8)\n",
    "input_batch = torch.randn(16, 3, 32, 32)\n",
    "output_batch = conv(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch function: just the code from the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batch =  torch.Size([16, 3, 32, 32])\n",
      "unfolded =  torch.Size([16, 27, 1024])\n",
      "X_reshaped =  torch.Size([16384, 27])\n",
      "W =  torch.Size([27, 8])\n",
      "Y =  torch.Size([16384, 8])\n",
      "Y_reshaped =  torch.Size([16, 1024, 8])\n",
      "Y_permuted =  torch.Size([16, 8, 1024])\n",
      "output_batch =  torch.Size([16, 8, 32, 32])\n",
      "grad_Y_permuted =  torch.Size([16, 8, 1024])\n",
      "grad_Y_reshaped =  torch.Size([16, 1024, 8])\n",
      "grad_Y =  torch.Size([16384, 8])\n",
      "grad_W =  torch.Size([27, 8])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_batch_grad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Temp\\YOES~1.YWE/ipykernel_20888/1454299165.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\function.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    197\u001b[0m                                \"of them.\")\n\u001b[0;32m    198\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Temp\\YOES~1.YWE/ipykernel_20888/1454299165.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(ctx, grad_output)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# a gradient (the stride and padding) you can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;31m# return None.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minput_batch_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[0minput_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_batch_grad' is not defined"
     ]
    }
   ],
   "source": [
    "class MyConv2DFunc(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward\n",
    "    passes which operate on Tensors.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, stride=1, padding=1):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input\n",
    "        and return a Tensor containing the output. ctx is a context\n",
    "        object that can be used to stash information for backward\n",
    "        computation. You can cache arbitrary objects for use in the\n",
    "        backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        output_channels = kernel.size()[1]\n",
    "        \n",
    "        # your code here\n",
    "        batch_size, input_channels, height, width = input_batch.size()\n",
    "        print(\"input_batch = \", input_batch.size())\n",
    "        \n",
    "        kernel_size = int(math.sqrt(kernel.size()[0]/input_channels))\n",
    "        \n",
    "        # output dimensions\n",
    "        h_out = int(output_size(input_batch, kernel_size, stride, padding))\n",
    "        w_out = int(output_size(input_batch, kernel_size, stride, padding))\n",
    "        \n",
    "        # unfolded matrix (b, k, p)\n",
    "        U = F.unfold(input_batch, (kernel_size, kernel_size), padding=padding, stride=stride)\n",
    "        print(\"unfolded = \", unfolded.size())\n",
    "        batch_size, k_values_per_patch, patches = unfolded.size()\n",
    "        \n",
    "        # reshape to (b, p, k) tensor, than merge b and p to get (b*p,k) tensor\n",
    "        reshaped_X = torch.transpose(U, 1, 2).reshape(-1, k_values_per_patch)\n",
    "        print(\"X_reshaped = \", reshaped_X.size())\n",
    "        \n",
    "        # Initiate random weights with correct dimensions\n",
    "        W = torch.rand((k_values_per_patch, output_channels)) # - rows: number of nodes in one patch of input. -columns: # of nodes in one pixel in output\n",
    "        print(\"W = \", W.size())\n",
    "    \n",
    "        # store objects for the backward\n",
    "        ctx.save_for_backward(input_batch, reshaped_X, W)\n",
    "        \n",
    "        # Matrix multiplication to get Y\n",
    "        Y = torch.mm(reshaped_X, W) # bmm?\n",
    "        print(\"Y = \", Y.size())\n",
    "        \n",
    "        # Reshape to get seperate batches back\n",
    "        Y_reshaped = Y.reshape((batch_size, patches, output_channels)) # contains one row-vector for each pixel in output\n",
    "        print(\"Y_reshaped = \", Y_reshaped.size())\n",
    "        \n",
    "        # Permute to swap axis for p and k\n",
    "        Y_permuted = torch.permute(Y_reshaped, (0, 2, 1))\n",
    "        print(\"Y_permuted = \", Y_permuted.size())\n",
    "        \n",
    "        output_batch = Y_permuted.reshape(batch_size, output_channels, h_out, w_out)\n",
    "        print(\"output_batch = \", output_batch.size())\n",
    "        \n",
    "        assert output_batch.size() == torch.nn.functional.conv2d(input_batch, W.reshape(output_channels, input_channels, kernel_size, kernel_size),padding=padding).size()\n",
    "        return output_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the\n",
    "        gradient of the loss with respect to the output, and we need\n",
    "        to compute the gradient of the loss with respect to the\n",
    "        input\n",
    "        \"\"\"\n",
    "        # retrieve stored objects\n",
    "        input_batch, reshaped_X, W = ctx.saved_tensors\n",
    "        # your code here\n",
    "        \n",
    "        grad_Y_permuted = grad_output.reshape(grad_output.size()[0], grad_output.size()[1], grad_output.size()[2]*grad_output.size()[2])\n",
    "        print(\"grad_Y_permuted = \", grad_Y_permuted.size())\n",
    "        grad_Y_reshaped = torch.permute(grad_Y_permuted, (0, 2, 1))\n",
    "        print(\"grad_Y_reshaped = \", grad_Y_reshaped.size())\n",
    "        grad_Y = grad_Y_reshaped.reshape(grad_Y_reshaped.size()[0]*grad_Y_reshaped.size()[1],grad_Y_reshaped.size()[2])\n",
    "        print(\"grad_Y = \", grad_Y.size())\n",
    "        grad_W = torch.transpose(torch.mm(torch.transpose(grad_Y,0,1), reshaped_X),0,1)\n",
    "        print(\"grad_W = \", grad_W.size())\n",
    "        grad_U = torch.transpose(torch.mm(grad_Y, torch.transpose(W,0,1)),0,1)\n",
    "        \n",
    "        # The gradients of the inputs. For anything that doesn't have\n",
    "        # a gradient (the stride and padding) you can\n",
    "        # return None.\n",
    "        return input_batch_grad, kernel_grad, None, None\n",
    "        \n",
    "input_channels = 3\n",
    "output_channels = 8\n",
    "kernel_size = 3\n",
    "\n",
    "input_batch = torch.randn(16, 3, 32, 32, requires_grad=True)\n",
    "kernel = torch.randn(kernel_size*kernel_size*input_channels, output_channels, requires_grad=True)\n",
    "\n",
    "conv = MyConv2DFunc.apply\n",
    "output = conv(input_batch, kernel)\n",
    "loss = output.sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "arg = {\"data\":'./data', \"batch\": 60000} # with batch = 16 we get a dataloader of length 3750 (*16=60.000)\n",
    "train = torchvision.datasets.MNIST(root=arg['data'], train=True, download=True, transform=ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=arg['batch'], shuffle=True, num_workers=2)\n",
    "test = torchvision.datasets.MNIST(root=arg['data'], train=False, download=True, transform=ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=arg['batch'], shuffle=True, num_workers=2)\n",
    "\n",
    "for i, data in enumerate(trainloader):\n",
    "    input, labels = data\n",
    "\n",
    "#ongeveer 15sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = input[:50000]\n",
    "training_label = labels[:50000]\n",
    "validation = input[50000:]\n",
    "validation_label = labels[50000:]\n",
    "\n",
    "def loop_over(data, label, step):\n",
    "    for i in range(0, len(data),step):\n",
    "        data[i:i+step]\n",
    "        label[i:i+step]\n",
    "\n",
    "loop_over(training, training_label, 16)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8:\n",
    "Build this network and tune the hyperparameters until you get a good baseline\n",
    "performance you are happy with. You should be able to get at least 95% accuracy. If training\n",
    "takes too long, you can reduce the number of channels in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# vanuit de blitz tutorial\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "conv2d(tensor, weight, stride=2,padding=0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e2d738271302b869834cae786a1c397433e224a04daa139e6e226c81326f323"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
