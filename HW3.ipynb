{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: \n",
    "Write a pseudo-code for how you would implement this with a set of nested\n",
    "for loops. The convolution is defined by a set of weights/parameters which we will learn.\n",
    "How do you represent these weights?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in batch_size:\n",
    "#     padding_width = input_width + 2*padding\n",
    "#     padding_height = input_height + 2*padding\n",
    "#     # Given that output_width and output_height are the same:\n",
    "#     output_width = output_height = ((input_width - kernel_size + 2*padding)/stride) + 1\n",
    "#     # Create empty output for this layer\n",
    "#     output = zeros([total_kernels, output_width, output_height])\n",
    "#     for layer in input_channels:\n",
    "#         layer_pad = [sample, layer, :, :]\n",
    "#         # Add padding\n",
    "#         for pad in padding:\n",
    "#             add column of zeros to right to layer_pad\n",
    "#             add column of zeros to left to layer_pad\n",
    "#             add column of zeros to top to layer_pad\n",
    "#             add column of zeros to bottom to layer_pad\n",
    "#         # Move over layer:\n",
    "#         for y in range(output_height-1):\n",
    "#             for x in range(output_width-1):\n",
    "#                 # If multiple kernels exist, save in different slices of output\n",
    "#                 for kernel in total_kernels:\n",
    "#                     patch = layer_pad[y*stride : (y*stride)+kernel_size, x*stride : (x*stride)+kernel_size]\n",
    "#                     output[kernel, y, x] += sum( patch * kernel[layer, :, :] )\n",
    "#     sample_output[sample, :, :, :] = output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. \n",
    "For a given input tensor, kernel size, stride and padding (no dilutions) work out\n",
    "a general function that computes the size of the output.\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html?highlight=functional%20conv2d#torch.nn.functional.conv2d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import ipykernel\n",
    "ipykernel.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n",
      "torch.Size([1, 1, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "tensor = np.random.rand(1, 2, 27, 27)\n",
    "weight = np.random.rand(1, 2, 3, 3)\n",
    "\n",
    "def output_size(input_tensor, kernel_size, stride, padding):\n",
    "    batch_size, channels, height, width = input_tensor.shape # <- similar to tensor.size() in torch, but '.shape' here since it is numpy\n",
    "    out_size = (((height - kernel_size + 2*padding) / stride ) + 1)\n",
    "    return out_size\n",
    "\n",
    "print(output_size(tensor, 3, 2, 0)) #<- example from the slides: Lecture 3 AlexNet, top right\n",
    "\n",
    "tensor = torch.rand([1, 2, 27, 27])\n",
    "weight = torch.rand([1, 2, 3, 3])\n",
    "\n",
    "print(torch.nn.functional.conv2d(tensor, weight, stride=2,padding=0).size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: \n",
    "Write a naive (non-vectorized) implementation of the unfold function in\n",
    "pseudocode. Include the pseudocode in your report.\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.unfold.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #input_tensor = [b, c, h, w] #batch_size, channels, height, width\n",
    "\n",
    "# # Pseudo code naive unfold:\n",
    "# def naive_unfold(input_tensor, kernel_size, stride, padding):\n",
    "#     output_size = output_size(input_tensor, kernel_size, stride, padding)\n",
    "#     #1 extract all patches from the input\n",
    "#     for sample in b:\n",
    "#         for channel in c:\n",
    "#             layer_pad = [sample, layer, :, :]\n",
    "#             # Add padding\n",
    "#             for pad in padding:\n",
    "#                 add column of zeros to right to layer_pad\n",
    "#                 add column of zeros to left to layer_pad\n",
    "#                 add column of zeros to top to layer_pad\n",
    "#                 add column of zeros to bottom to layer_pad\n",
    "#             n_patches_per_layer = output_size * output_size\n",
    "#             for y in range(output_size-1):\n",
    "#                 for x in range(output_size-1):\n",
    "#                     # x+ y = number of patch, total_patch is 0 first time\n",
    "#                     total_patch = (x+y) * channel\n",
    "#                     patch[sample, x+y + total_patch ,:, :] = layer_pad[y*stride : (y*stride)+kernel_size, \n",
    "#                                                                         x*stride : (x*stride)+kernel_size]\n",
    "#     # 2. Flatten these patches (with all channels) into vectors, arranged as the columns of a matrix X.\n",
    "#     #### THIS CORRECT???\n",
    "#     X = patch[:, :].flatten()\n",
    "#     p = len(X)\n",
    "#     # 3. Multiply this matrix by a weight matrix Y = XW\n",
    "#     Y = X * W\n",
    "#     # 4. Reshape the matrix Y, so that its columns become the pixels of the output tensor.\n",
    "#     k = c * output_size * output_size\n",
    "#     Y = Y.reshape([b, k, p])\n",
    "#     return patch\n",
    "#     #output = [b, k, p] #batch_size, number of values per patch, number of patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch Module : niet af!\n",
    "Inspiratie (kreeg ik doorgestuurd): \n",
    "https://discuss.pytorch.org/t/decompose-conv2d-input-unfold-gemm-fold/93740 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batch =  torch.Size([16, 3, 32, 32])\n",
      "unfolded =  torch.Size([16, 27, 1024])\n",
      "reshaped =  torch.Size([432, 1024])\n",
      "W =  torch.Size([1024, 1024])\n",
      "Y =  torch.Size([432, 1024])\n",
      "torch.Size([16, 16, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels, kernel_size=(3,3), stride=1, padding=1):\n",
    "        super().__init__() # <- belangrijk!\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "    def forward(self, input_batch):\n",
    "\n",
    "        \n",
    "        batch_size, channels, height, width = input_batch.size()\n",
    "        print(\"input_batch = \", input_batch.size())\n",
    "        # your code here\n",
    "        unfolded = F.unfold(input_batch, self.kernel_size, padding=self.padding, stride=self.stride)\n",
    "        print(\"unfolded = \", unfolded.size())\n",
    "        # now (b, p, k)\n",
    "        batch_size, k_values_per_patch, patches = unfolded.size()\n",
    "        reshaped = unfolded.reshape((batch_size * k_values_per_patch, patches))\n",
    "        print(\"reshaped = \", reshaped.size())\n",
    "        W = torch.rand((patches, patches)) # - rows: number of nodes in one patch of input. -columns: # of nodes in one pixel in output\n",
    "        print(\"W = \", W.size())\n",
    "        Y = torch.matmul(reshaped, W) # bmm?\n",
    "        print(\"Y = \", Y.size())\n",
    "        rereshaped = Y.reshape((batch_size, k_values_per_patch, patches)) # contains one row-vector for each pixel in output\n",
    "\n",
    "        W = torch.rand([16, 3, 32, 32])\n",
    "\n",
    "        print(torch.nn.functional.conv2d(input_batch, W, stride=self.stride,padding=self.padding).size())\n",
    "\n",
    "\n",
    "        # permutation\n",
    "        # reshape\n",
    "        return rereshaped\n",
    "\n",
    "# We use the Conv2D module by instantiating it, and applying it to an input.\n",
    "conv = Conv2D(in_channels= 3, out_channels= 8)\n",
    "input_batch = torch.randn(16, 3, 32, 32)\n",
    "output_batch = conv(input_batch)\n",
    "\n",
    "# by applying formula as described in output_size() the output_size should also be 32x32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch function: just the code from the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DFunc(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward\n",
    "    passes which operate on Tensors.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, stride=1, padding=1):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input\n",
    "        and return a Tensor containing the output. ctx is a context\n",
    "        object that can be used to stash information for backward\n",
    "        computation. You can cache arbitrary objects for use in the\n",
    "        backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        # store objects for the backward\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.save_for_backward(kernel)\n",
    "        # your code here\n",
    "        ...\n",
    "        return output_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the\n",
    "        gradient of the loss with respect to the output, and we need\n",
    "        to compute the gradient of the loss with respect to the\n",
    "        input\n",
    "        \"\"\"\n",
    "        # retrieve stored objects\n",
    "        input, = ctx.saved_tensors\n",
    "        # your code here\n",
    "        ...\n",
    "        # The gradients of the inputs. For anything that doesn't have\n",
    "        # a gradient (the stride and padding) you can\n",
    "        # return None.\n",
    "        return input_batch_grad, kernel_grad, None, None\n",
    "        \n",
    "input_batch = torch.randn(16, 3, 32, 32)\n",
    "kernel = torch.randn(...)\n",
    "Conv2DFunc.apply(input_batch, kernel)\n",
    "output_batch = conv(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "arg = {\"data\":'./data', \"batch\": 60000} # with batch = 16 we get a dataloader of length 3750 (*16=60.000)\n",
    "train = torchvision.datasets.MNIST(root=arg['data'], train=True, download=True, transform=ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=arg['batch'], shuffle=True, num_workers=2)\n",
    "test = torchvision.datasets.MNIST(root=arg['data'], train=False, download=True, transform=ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=arg['batch'], shuffle=True, num_workers=2)\n",
    "\n",
    "for i, data in enumerate(trainloader):\n",
    "    input, labels = data\n",
    "\n",
    "#ongeveer 15sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = input[:50000]\n",
    "training_label = labels[:50000]\n",
    "validation = input[50000:]\n",
    "validation_label = labels[50000:]\n",
    "\n",
    "def loop_over(data, label, step):\n",
    "    for i in range(0, len(data),step):\n",
    "        data[i:i+step]\n",
    "        label[i:i+step]\n",
    "\n",
    "loop_over(training, training_label, 16)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8:\n",
    "Build this network and tune the hyperparameters until you get a good baseline\n",
    "performance you are happy with. You should be able to get at least 95% accuracy. If training\n",
    "takes too long, you can reduce the number of channels in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# vanuit de blitz tutorial\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "conv2d(tensor, weight, stride=2,padding=0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e2d738271302b869834cae786a1c397433e224a04daa139e6e226c81326f323"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('myenv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
