{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in batch_size:\n",
    "#     padding_width = input_width + 2*padding\n",
    "#     padding_height = input_height + 2*padding\n",
    "#     # Given that output_width and output_height are the same:\n",
    "#     output_width = output_height = ((input_width - kernel_size + 2*padding)/stride) + 1\n",
    "#     # Create empty output for this layer\n",
    "#     output = zeros([total_kernels, output_width, output_height])\n",
    "#     for layer in input_channels:\n",
    "#         layer_pad = [sample, layer, :, :]\n",
    "#         # Add padding\n",
    "#         for pad in padding:\n",
    "#             add column of zeros to right to layer_pad\n",
    "#             add column of zeros to left to layer_pad\n",
    "#             add column of zeros to top to layer_pad\n",
    "#             add column of zeros to bottom to layer_pad\n",
    "#         # Move over layer:\n",
    "#         for y in range(output_height-1):\n",
    "#             for x in range(output_width-1):\n",
    "#                 # If multiple kernels exist, save in different slices of output\n",
    "#                 for kernel in total_kernels:\n",
    "#                     patch = layer_pad[y*stride : (y*stride)+kernel_size, x*stride : (x*stride)+kernel_size]\n",
    "#                     output[kernel, y, x] += sum( patch * kernel[layer, :, :] )\n",
    "#     sample_output[sample, :, :, :] = output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. \n",
    "For a given input tensor, kernel size, stride and padding (no dilutions) work out\n",
    "a general function that computes the size of the output.\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html?highlight=functional%20conv2d#torch.nn.functional.conv2d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import ipykernel\n",
    "import math\n",
    "ipykernel.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n",
      "torch.Size([1, 1, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "tensor = np.random.rand(1, 2, 27, 27)\n",
    "weight = np.random.rand(1, 2, 3, 3)\n",
    "\n",
    "def output_size(input_tensor, kernel_size, stride, padding):\n",
    "    batch_size, channels, height, width = input_tensor.shape # <- similar to tensor.size() in torch, but '.shape' here since it is numpy\n",
    "    out_size = (((height - kernel_size + 2*padding) / stride ) + 1)\n",
    "    return out_size\n",
    "\n",
    "print(output_size(tensor, 3, 2, 0)) #<- example from the slides: Lecture 3 AlexNet, top right\n",
    "\n",
    "tensor = torch.rand([1, 2, 27, 27])\n",
    "weight = torch.rand([1, 2, 3, 3])\n",
    "\n",
    "print(torch.nn.functional.conv2d(tensor, weight, stride=2,padding=0).size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: \n",
    "Write a naive (non-vectorized) implementation of the unfold function in\n",
    "pseudocode. Include the pseudocode in your report.\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.unfold.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #input_tensor = [b, c, h, w] #batch_size, channels, height, width\n",
    "\n",
    "# # Pseudo code naive unfold:\n",
    "# def naive_unfold(input_tensor, kernel_size, stride, padding):\n",
    "#     output_size = output_size(input_tensor, kernel_size, stride, padding)\n",
    "#     #1 extract all patches from the input\n",
    "#     for sample in b:\n",
    "#         for channel in c:\n",
    "#             layer_pad = [sample, layer, :, :]\n",
    "#             # Add padding\n",
    "#             for pad in padding:\n",
    "#                 add column of zeros to right to layer_pad\n",
    "#                 add column of zeros to left to layer_pad\n",
    "#                 add column of zeros to top to layer_pad\n",
    "#                 add column of zeros to bottom to layer_pad\n",
    "#             n_patches_per_layer = output_size * output_size\n",
    "#             for y in range(output_size-1):\n",
    "#                 for x in range(output_size-1):\n",
    "#                     # x+ y = number of patch, total_patch is 0 first time\n",
    "#                     total_patch = (x+y) * channel\n",
    "#                     patch[sample, x+y + total_patch ,:, :] = layer_pad[y*stride : (y*stride)+kernel_size, \n",
    "#                                                                         x*stride : (x*stride)+kernel_size]\n",
    "#     # 2. Flatten these patches (with all channels) into vectors, arranged as the columns of a matrix X.\n",
    "#     #### THIS CORRECT???\n",
    "#     X = patch[:, :].flatten()\n",
    "#     p = len(X)\n",
    "#     # 3. Multiply this matrix by a weight matrix Y = XW\n",
    "#     Y = X * W\n",
    "#     # 4. Reshape the matrix Y, so that its columns become the pixels of the output tensor.\n",
    "#     k = c * output_size * output_size\n",
    "#     Y = Y.reshape([b, k, p])\n",
    "#     return patch\n",
    "#     #output = [b, k, p] #batch_size, number of values per patch, number of patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batch =  torch.Size([16, 3, 32, 32])\n",
      "unfolded =  torch.Size([16, 27, 1024])\n",
      "X_reshaped =  torch.Size([16384, 27])\n",
      "W =  torch.Size([27, 8])\n",
      "Y =  torch.Size([16384, 8])\n",
      "Y_reshaped =  torch.Size([16, 1024, 8])\n",
      "Y_permuted =  torch.Size([16, 8, 1024])\n",
      "output =  torch.Size([16, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels, kernel_size=(3,3), stride=1, padding=1):\n",
    "        super().__init__() # <- belangrijk!\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        batch_size, channels, height, width = input_batch.size()\n",
    "        print(\"input_batch = \", input_batch.size())\n",
    "        \n",
    "        # output dimensions\n",
    "        h_out = int(output_size(input_batch, self.kernel_size[0], self.stride, self.padding))\n",
    "        w_out = int(output_size(input_batch, self.kernel_size[1], self.stride, self.padding))\n",
    "        \n",
    "        # unfolded matrix (b, k, p)\n",
    "        unfolded = F.unfold(input_batch, self.kernel_size, padding=self.padding, stride=self.stride)\n",
    "        print(\"unfolded = \", unfolded.size())\n",
    "        batch_size, k_values_per_patch, patches = unfolded.size()\n",
    "        \n",
    "        # reshape to (b, p, k) tensor, than merge b and p to get (b*p,k) tensor\n",
    "        reshaped = torch.transpose(unfolded, 1, 2).reshape(-1, k_values_per_patch)\n",
    "        print(\"X_reshaped = \", reshaped.size())\n",
    "        \n",
    "        # Initiate random weights with correct dimensions\n",
    "        W = torch.rand((k_values_per_patch, self.out_channels)) # - rows: number of nodes in one patch of input. -columns: # of nodes in one pixel in output\n",
    "        print(\"W = \", W.size())\n",
    "        \n",
    "        # Matrix multiplication to get Y\n",
    "        Y = torch.mm(reshaped, W) # bmm?\n",
    "        print(\"Y = \", Y.size())\n",
    "        \n",
    "        # Reshape to get seperate batches back\n",
    "        Y_reshaped = Y.reshape((batch_size, patches, self.out_channels)) # contains one row-vector for each pixel in output\n",
    "        print(\"Y_reshaped = \", Y_reshaped.size())\n",
    "        \n",
    "        # Permute to swap axis for p and k\n",
    "        Y_permuted = torch.permute(Y_reshaped, (0, 2, 1))\n",
    "        print(\"Y_permuted = \", Y_permuted.size())\n",
    "        \n",
    "        # Fold back to obtain the output of this layer\n",
    "        output = Y_permuted.reshape(batch_size, self.out_channels, h_out, w_out)\n",
    "        print(\"output = \", output.size())\n",
    "        \n",
    "        assert output.size() == torch.nn.functional.conv2d(input_batch, W.reshape(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1]),padding=self.padding).size()\n",
    "        return output\n",
    "\n",
    "# We use the Conv2D module by instantiating it, and applying it to an input.\n",
    "torch.manual_seed(0)\n",
    "conv = Conv2D(in_channels= 3, out_channels= 8)\n",
    "input_batch = torch.randn(16, 3, 32, 32)\n",
    "output_batch = conv(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4, 5 & 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  torch.Size([16, 3, 32, 32])\n",
      "U =  torch.Size([16, 27, 1024])\n",
      "U_reshaped =  torch.Size([16384, 27])\n",
      "W =  torch.Size([27, 8])\n",
      "Y =  torch.Size([16384, 8])\n",
      "Y_reshaped =  torch.Size([16, 1024, 8])\n",
      "Y_permuted =  torch.Size([16, 8, 1024])\n",
      "output_batch =  torch.Size([16, 8, 32, 32])\n",
      "\n",
      "\n",
      "grad_Y_permuted =  torch.Size([16, 8, 1024])\n",
      "grad_Y_reshaped =  torch.Size([16, 1024, 8])\n",
      "grad_Y =  torch.Size([16384, 8])\n",
      "grad_W =  torch.Size([27, 8])\n",
      "grad_U_reshaped =  torch.Size([16384, 27])\n",
      "grad_U =  torch.Size([16, 27, 1024])\n",
      "grad_X =  torch.Size([16, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "class MyConv2DFunc(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward\n",
    "    passes which operate on Tensors.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, stride=1, padding=1):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input\n",
    "        and return a Tensor containing the output. ctx is a context\n",
    "        object that can be used to stash information for backward\n",
    "        computation. You can cache arbitrary objects for use in the\n",
    "        backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        output_channels = kernel.size()[1]\n",
    "        \n",
    "        # your code here\n",
    "        batch_size, input_channels, height, width = input_batch.size()\n",
    "        print(\"X = \", input_batch.size())\n",
    "        \n",
    "        kernel_size = int(math.sqrt(kernel.size()[0]/input_channels))\n",
    "        \n",
    "        # output dimensions\n",
    "        h_out = int(output_size(input_batch, kernel_size, stride, padding))\n",
    "        w_out = int(output_size(input_batch, kernel_size, stride, padding))\n",
    "        \n",
    "        # unfolded matrix (b, k, p)\n",
    "        U = F.unfold(input_batch, (kernel_size, kernel_size), padding=padding, stride=stride)\n",
    "        print(\"U = \", U.size())\n",
    "        batch_size, k_values_per_patch, patches = U.size()\n",
    "        \n",
    "        # reshape to (b, p, k) tensor, than merge b and p to get (b*p,k) tensor\n",
    "        U_reshaped = torch.transpose(U, 1, 2).reshape(-1, k_values_per_patch)\n",
    "        print(\"U_reshaped = \", U_reshaped.size())\n",
    "        \n",
    "        # Initiate random weights with correct dimensions\n",
    "        W = torch.rand((k_values_per_patch, output_channels)) # - rows: number of nodes in one patch of input. -columns: # of nodes in one pixel in output\n",
    "        print(\"W = \", W.size())\n",
    "    \n",
    "        # store objects for the backward\n",
    "        ctx.save_for_backward(input_batch, U_reshaped, W)\n",
    "        \n",
    "        # Matrix multiplication to get Y\n",
    "        Y = torch.mm(U_reshaped, W) # bmm?\n",
    "        print(\"Y = \", Y.size())\n",
    "        \n",
    "        # Reshape to get seperate batches back\n",
    "        Y_reshaped = Y.reshape((batch_size, patches, output_channels)) # contains one row-vector for each pixel in output\n",
    "        print(\"Y_reshaped = \", Y_reshaped.size())\n",
    "        \n",
    "        # Permute to swap axis for p and k\n",
    "        Y_permuted = torch.permute(Y_reshaped, (0, 2, 1))\n",
    "        print(\"Y_permuted = \", Y_permuted.size())\n",
    "        \n",
    "        output_batch = Y_permuted.reshape(batch_size, output_channels, h_out, w_out)\n",
    "        print(\"output_batch = \", output_batch.size())\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        assert output_batch.size() == torch.nn.functional.conv2d(input_batch, W.reshape(output_channels, input_channels, kernel_size, kernel_size),padding=padding).size()\n",
    "        return output_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the\n",
    "        gradient of the loss with respect to the output, and we need\n",
    "        to compute the gradient of the loss with respect to the\n",
    "        input\n",
    "        \"\"\"\n",
    "        # retrieve stored objects\n",
    "        input_batch, X_reshaped, W = ctx.saved_tensors\n",
    "        # your code here\n",
    "        \n",
    "        grad_Y_permuted = grad_output.reshape(grad_output.size()[0], grad_output.size()[1], grad_output.size()[2]*grad_output.size()[2])\n",
    "        print(\"grad_Y_permuted = \", grad_Y_permuted.size())\n",
    "        grad_Y_reshaped = torch.permute(grad_Y_permuted, (0, 2, 1))\n",
    "        print(\"grad_Y_reshaped = \", grad_Y_reshaped.size())\n",
    "        grad_Y = grad_Y_reshaped.reshape(grad_Y_reshaped.size()[0]*grad_Y_reshaped.size()[1],grad_Y_reshaped.size()[2])\n",
    "        print(\"grad_Y = \", grad_Y.size())\n",
    "        kernel_grad = torch.transpose(torch.mm(torch.transpose(grad_Y,0,1), X_reshaped),0,1)\n",
    "        print(\"grad_W = \", kernel_grad.size())\n",
    "        grad_U_reshaped = torch.mm(grad_Y, torch.transpose(W,0,1))\n",
    "        print(\"grad_U_reshaped = \", grad_U_reshaped.size())\n",
    "        grad_U = torch.permute(grad_U_reshaped.reshape(input_batch.size()[0], int(grad_U_reshaped.size()[0]/input_batch.size()[0]), kernel_grad.size()[0]), (0, 2, 1))\n",
    "        print(\"grad_U = \", grad_U.size())\n",
    "        input_batch_grad = F.fold(grad_U, output_size=[input_batch.size()[2], input_batch.size()[3]], kernel_size=(3,3), padding=1)\n",
    "        print(\"grad_X = \", input_batch_grad.size())\n",
    "        return input_batch_grad, kernel_grad, None, None\n",
    "        \n",
    "input_channels = 3\n",
    "output_channels = 8\n",
    "kernel_size = 3\n",
    "\n",
    "input_batch = torch.randn(16, 3, 32, 32, requires_grad=True)\n",
    "kernel = torch.randn(kernel_size*kernel_size*input_channels, output_channels, requires_grad=True)\n",
    "\n",
    "conv = MyConv2DFunc.apply\n",
    "output = conv(input_batch, kernel)\n",
    "loss = output.sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "arg = {\"data\":'./data', \"batch\": 60000} # with batch = 16 we get a dataloader of length 3750 (*16=60.000)\n",
    "training_data = torchvision.datasets.MNIST(root=arg['data'], train=True, download=True, transform=ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(training_data, batch_size=arg['batch'], shuffle=True, num_workers=2)\n",
    "test_set = torchvision.datasets.MNIST(root=arg['data'], train=False, download=True, transform=ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=arg['batch'], shuffle=True, num_workers=2)\n",
    "\n",
    "for i, data in enumerate(trainloader):\n",
    "    input, labels = data\n",
    "\n",
    "#ongeveer 15sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = (input[:50000])\n",
    "training_label = labels[:50000]\n",
    "validation = input[50000:]\n",
    "validation_label = labels[50000:]\n",
    "\n",
    "def loop_over(data, label, step):\n",
    "    for i in range(0,len(data),step):\n",
    "        batch = data[i:i+step]\n",
    "        batch_labels = label[i:i+step]\n",
    "\n",
    "loop_over(training, training_label, 16)\n",
    "loop_over(validation, validation_label, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "Build this network and tune the hyperparameters until you get a good baseline\n",
    "performance you are happy with. You should be able to get at least 95% accuracy. If training\n",
    "takes too long, you can reduce the number of channels in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "from torchvision.transforms import ToTensor, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_chan, kernel, stride, padding, output):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(input_chan, 16, kernel, stride, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, kernel, stride, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(32, 64, kernel, stride, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.out = nn.Linear(64*3*3, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size):\n",
    "    loaders = {'train_set' : DataLoader(train_set, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "               'val_set' : DataLoader(val_set, \n",
    "                                          batch_size=10000, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "                'test_set'  : DataLoader(test_set, \n",
    "                                          batch_size=len(test_set), \n",
    "                                          shuffle=False, \n",
    "                                          num_workers=1)}\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, loaders):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loaders['val_set']:\n",
    "            output = net(x)\n",
    "            _,pred_y = torch.max(output, dim = 1)\n",
    "            correct += (pred_y == y).float().sum()\n",
    "\n",
    "    print('accuracy on validation set', (correct / 10000)*100, '%')\n",
    "    return (correct / 10000)*100\n",
    "\n",
    "def train(net, loaders, epochs, loss_f, opt):\n",
    "    train_loss = []\n",
    "    epoch_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('epoch = ', i)\n",
    "        for j, (x, y) in enumerate(loaders['train_set']):\n",
    "            opt.zero_grad()\n",
    "            x_batch = x\n",
    "            y_batch = y\n",
    "            output = net.forward(x_batch)\n",
    "            loss = loss_f(output, y_batch)\n",
    "            train_loss.append(loss)\n",
    "            if j % 1000 == 0:\n",
    "                print('loss:', loss.item())\n",
    "            loss.backward()\n",
    "            opt.step() \n",
    "        epoch_list.append(i)\n",
    "        acc_list.append(validate(net, loaders))\n",
    "    return train_loss, epoch_list, acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_chan = 1\n",
    "# output = 10\n",
    "# kernel = 3\n",
    "# stride = 1\n",
    "# padding = 1\n",
    "# lr = 0.0001\n",
    "# net = Net(input_chan, kernel, stride, padding, output)\n",
    "# loss_f = nn.CrossEntropyLoss()\n",
    "# opt = optim.Adam(net.parameters(), lr)\n",
    "\n",
    "\n",
    "# epochs = 5\n",
    "# batch_size = 16\n",
    "# train_set, val_set = torch.utils.data.random_split(training_data, [50000, 10000])\n",
    "# loaders = get_loaders(batch_size) \n",
    "# train_loss, epoch_list, acc_list = train(net, loaders, epochs, loss_f, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y1 = acc_list\n",
    "# x1 = epoch_list\n",
    "\n",
    "# plt.plot(x1, y1, label = \"batch_size = 16\" )\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Accuracy during training epochs')\n",
    "# plt.xticks(np.arange(0, 20, 1))\n",
    "# plt.savefig('figures/Q8_acc')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = transforms.Compose(\n",
    "                    [\n",
    "                    transforms.RandomRotation((-7.0,7.0),fill=(1,)),\n",
    "                    # transforms.RandomAffine((-3,3),translate =(0.05,0.01)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1308,), (0.3016,)) \n",
    "                    ])\n",
    "\n",
    "training_data = torchvision.datasets.MNIST(root=arg['data'], train=True, download=True, transform=augmented)\n",
    "train_set, val_set = torch.utils.data.random_split(training_data, [50000, 10000])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_chan = 1\n",
    "# output = 10\n",
    "# kernel = 3\n",
    "# stride = 1\n",
    "# padding = 1\n",
    "# lr = 0.0001\n",
    "# net = Net(input_chan, kernel, stride, padding, output)\n",
    "# loss_f = nn.CrossEntropyLoss()\n",
    "# opt = optim.Adam(net.parameters(), lr)\n",
    "\n",
    "# epochs = 5\n",
    "# batch_size = 16\n",
    "# loaders = get_loaders(batch_size) \n",
    "# train_loss, epoch_list, acc_list = train(net, loaders, epochs, loss_f, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(epoch_list, acc_list, label = \"batch_size = 16\" )\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Accuracy during training epochs')\n",
    "# plt.xticks(np.arange(0, 20, 1))\n",
    "# plt.savefig('Q9_acc_augmented')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 511, 383])\n",
      "torch.Size([1, 16, 959, 539])\n",
      "ERROR: Dimensions are not correct..\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.rand(1, 3, 1024, 768)\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride = 2, padding=1)\n",
    "print(conv_layer(input_tensor).size())\n",
    "\n",
    "input_tensor = torch.rand(1, 3, 1920, 1080)\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride = 2, padding=1)\n",
    "print(conv_layer(input_tensor).size())\n",
    "\n",
    "input_tensor = torch.rand(1, 8, 1920, 1080)\n",
    "try:\n",
    "    conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride = 2, padding=1)\n",
    "    print(conv_layer(input_tensor).size())\n",
    "except:\n",
    "    print(\"ERROR: Dimensions are not correct..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 1, 1])\n",
      "torch.Size([16, 3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "b = 16\n",
    "c = 3\n",
    "h = 32\n",
    "w = 32\n",
    "\n",
    "input_tensor = torch.rand((b, c, h, w))\n",
    "global_average_pooling = torch.nn.AvgPool2d((h,w), stride=None, padding=0)\n",
    "global_max_pooling = torch.nn.MaxPool2d((h,w), stride=None, padding=0)\n",
    "avg_pooled = global_average_pooling(input_tensor)  \n",
    "max_pooled = global_max_pooling(input_tensor)\n",
    "print(avg_pooled.size())\n",
    "print(max_pooled.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_size = transforms.Compose(\n",
    "                    [\n",
    "                    transforms.Resize((28,28)),\n",
    "                    # transforms.RandomRotation((-7.0,7.0),fill=(1,)),\n",
    "                    # transforms.RandomAffine((-3,3),translate =(0.02,0.02)),\n",
    "                    transforms.ToTensor(),\n",
    "                    # transforms.Normalize((0.1308,), (0.3016,)) \n",
    "                    ])\n",
    "\n",
    "training_data = torchvision.datasets.ImageFolder('mnist-varres/train', transform=one_size)\n",
    "test_set= torchvision.datasets.ImageFolder('mnist-varres/test', transform=one_size)\n",
    "train_set, val_set = torch.utils.data.random_split(training_data, [50000, 10000]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(training_data, batch_size=arg['batch'], shuffle=True, num_workers=2)\n",
    "for i, data in enumerate(trainloader):\n",
    "    input, labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_chan = 3   ## I DON\"T KNOW WHY IT HAS 3 INPUT CHANNELS HERE (NOWHERE MENTIONED)\n",
    "# output = 10\n",
    "# kernel = 3\n",
    "# stride = 1\n",
    "# padding = 1\n",
    "# lr = 0.0001\n",
    "# net = Net(input_chan, kernel, stride, padding, output)\n",
    "# loss_f = nn.CrossEntropyLoss()\n",
    "# opt = optim.Adam(net.parameters(), lr)\n",
    "\n",
    "# epochs = 5\n",
    "# batch_size = 16\n",
    "# loaders = get_loaders(batch_size) \n",
    "# train_loss, epoch_list, acc_list = train(net, loaders, epochs, loss_f, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Overleaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\n",
      "ERROR: No matching distribution found for cv2\n"
     ]
    }
   ],
   "source": [
    "!pip install cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import PIL\n",
    "import scipy.misc\n",
    "\n",
    "# All files ending with .png\n",
    "train_data = glob.glob(\"**/*png\")\n",
    "print(train_data)\n",
    "# Append pictures to list with their label\n",
    "size_32 = []\n",
    "size_48 = []\n",
    "size_64 = []\n",
    "for picture in train_data:\n",
    "    [label, name] = picture.split(\"\\\\\")\n",
    "    size = PIL.Image.open(picture).size[0]\n",
    "    print(scipy.misc.imread(picture, flatten=False, mode='RGB'))\n",
    "    if size == 32:\n",
    "        size_32.append((scipy.misc.imread(picture, flatten=False, mode='RGB'), label))\n",
    "    elif size == 48:\n",
    "        size_48.append((scipy.misc.imread(picture, flatten=False, mode='RGB'), label))\n",
    "    elif size == 64:\n",
    "        size_64.append((scipy.misc.imread(picture, flatten=False, mode='RGB'), label))\n",
    "\n",
    "data_32 = torch.tensor(size_32)\n",
    "data_48 = torch.tensor(size_48)\n",
    "data_64 = torch.tensor(size_64)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e2d738271302b869834cae786a1c397433e224a04daa139e6e226c81326f323"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
