{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: \n",
    "Write a pseudo-code for how you would implement this with a set of nested\n",
    "for loops. The convolution is defined by a set of weights/parameters which we will learn.\n",
    "How do you represent these weights?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in batch_size:\n",
    "    padding_width = input_width + 2*padding\n",
    "    padding_height = input_height + 2*padding\n",
    "    # Given that output_width and output_height are the same:\n",
    "    output_width = output_height = ((input_width - kernel_size + 2*padding)/stride) + 1\n",
    "    # Create empty output for this layer\n",
    "    output = zeros([total_kernels, output_width, output_height])\n",
    "    for layer in input_channels:\n",
    "        layer_pad = [sample, layer, :, :]\n",
    "        # Add padding\n",
    "        for pad in padding:\n",
    "            add column of zeros to right to layer_pad\n",
    "            add column of zeros to left to layer_pad\n",
    "            add column of zeros to top to layer_pad\n",
    "            add column of zeros to bottom to layer_pad\n",
    "        # Move over layer:\n",
    "        for y in range(output_height-1):\n",
    "            for x in range(output_width-1):\n",
    "                # If multiple kernels exist, save in different slices of output\n",
    "                for kernel in total_kernels:\n",
    "                    patch = layer_pad[y*stride : (y*stride)+kernel_size, x*stride : (x*stride)+kernel_size]\n",
    "                    output[kernel, y, x] += sum( patch * kernel[layer, :, :] )\n",
    "    sample_output[sample, :, :, :] = output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. \n",
    "For a given input tensor, kernel size, stride and padding (no dilutions) work out\n",
    "a general function that computes the size of the output.\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html?highlight=functional%20conv2d#torch.nn.functional.conv2d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n",
      "torch.Size([1, 1, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "tensor = np.random.rand(1, 2, 27, 27)\n",
    "weight = np.random.rand(1, 2, 3, 3)\n",
    "\n",
    "def output_size(input_tensor, kernel_size, stride, padding):\n",
    "    input_size = len(input_tensor[0,0,:])\n",
    "    out_size = (((input_size - kernel_size + 2*padding) / stride ) + 1)\n",
    "    return out_size\n",
    "\n",
    "print(output_size(tensor, 3, 2, 0)) #<- example from the slides: Lecture 3 AlexNet, top right\n",
    "\n",
    "tensor = torch.rand([1, 2, 27, 27])\n",
    "weight = torch.rand([1, 2, 3, 3])\n",
    "\n",
    "print(torch.nn.functional.conv2d(tensor, weight, stride=2,padding=0).size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: \n",
    "Write a naive (non-vectorized) implementation of the unfold function in\n",
    "pseudocode. Include the pseudocode in your report.\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.unfold.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_tensor = [b, c, h, w] #batch_size, channels, height, width\n",
    "\n",
    "# Pseudo code naive unfold:\n",
    "def naive_unfold(input_tensor, kernel_size, stride, padding):\n",
    "    channels = len(input_tensor[0, :])\n",
    "    width = len(input_tensor[0, 0, :])\n",
    "    padded_width = width + 2*padding\n",
    "    k = kernel_size * kernel_size * channels\n",
    "\n",
    "    output_size = output_size(input_tensor, kernel_size, stride, padding)\n",
    "\n",
    "    #1 extract all patches from the input\n",
    "    for sample in b:\n",
    "        for channel in c:\n",
    "            layer_pad = [sample, layer, :, :]\n",
    "            # Add padding\n",
    "            for pad in padding:\n",
    "                add column of zeros to right to layer_pad\n",
    "                add column of zeros to left to layer_pad\n",
    "                add column of zeros to top to layer_pad\n",
    "                add column of zeros to bottom to layer_pad\n",
    "            n_patches_per_layer = output_size * output_size\n",
    "            for y in range(output_size-1):\n",
    "                for x in range(output_size-1):\n",
    "                    # x+ y = number of patch, total_patch is 0 first time\n",
    "                    total_patch = (x+y) * channel\n",
    "                    patch[sample, x+y + total_patch ,:, :] = layer_pad[y*stride : (y*stride)+kernel_size, \n",
    "                                                                        x*stride : (x*stride)+kernel_size]\n",
    "    # 2. Flatten these patches (with all channels) into vectors, arranged as the columns of a matrix X.\n",
    "    #### THIS CORRECT???\n",
    "    X = patch[:, :].flatten()\n",
    "    p = len(X)\n",
    "    # 3. Multiply this matrix by a weight matrix Y = XW\n",
    "    Y = X * W\n",
    "    # 4. Reshape the matrix Y, so that its columns become the pixels of the output tensor.\n",
    "    k = c * output_size * output_size\n",
    "    Y = Y.reshape([b, k, p])\n",
    "    return patch\n",
    "    #output = [b, k, p] #batch_size, number of values per patch, number of patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Conv2D' object has no attribute '_backward_hooks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-13ab73968ec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0minput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0moutput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0m\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Conv2D' object has no attribute '_backward_hooks'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels, kernel_size=(3,3), stride=1, padding=1):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "    def forward(self, input_batch):\n",
    "\n",
    "        b, c, h, w = input_batch.size()\n",
    "        # your code here\n",
    "        unfold = F.unfold(input_batch, self.kernel_size, padding=self.padding, stride=self.stride)\n",
    "        print(unfold.size())\n",
    "        return unfold\n",
    "\n",
    "# We use the Conv2D module by instantiating it, and applying it to an input.\n",
    "conv = Conv2D(3, 1)\n",
    "input_batch = torch.randn(16, 3, 32, 32)\n",
    "output_batch = conv(input_batch)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e2d738271302b869834cae786a1c397433e224a04daa139e6e226c81326f323"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('myenv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
