{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions 14/15/16/17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_chan, kernel, strd, padding, output, pool):\n",
    "        super().__init__()\n",
    "        self.pool = pool\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(input_chan, 16, kernel, strd, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, kernel, strd, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(32, 81, kernel, strd, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.out = nn.Linear(81, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        if self.pool == 'avg':\n",
    "            x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "            # global_avg_pooling = torch.nn.AvgPool2d((x.shape[2],x.shape[3]))\n",
    "            # x = global_avg_pooling(x)\n",
    "            # x = torch.mean(x.view(x.size(0), x.size(1), -1), dim=2)\n",
    "            # x = nn.AdaptiveAvgPool2d(x, (1,1))\n",
    "            # x = F.avg_pool2d(x, (1, 1))\n",
    "        if self.pool == 'max':\n",
    "            x = F.max_pool2d(x, kernel_size=x.size()[2:])\n",
    "            # global_max_pooling = torch.nn.MaxPool2d((x.shape[2],x.shape[3]))\n",
    "            # x = global_max_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self, input_chan, kernel, strd, padding, output, pool):\n",
    "#         super().__init__()\n",
    "#         self.pool_c = pool\n",
    "#         self.pool = nn.MaxPool2d(2,2)\n",
    "#         self.conv1 = nn.Conv2d(input_chan, 16, kernel, strd, padding)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, kernel, strd, padding)\n",
    "#         self.conv3 = nn.Conv2d(32, 81, kernel, strd, padding)\n",
    "#         self.out = nn.Linear(81, output)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = self.pool(F.relu(self.conv3(x)))\n",
    "#         if self.pool_C == 'max':\n",
    "#             x = F.max_pool2d(x, kernel_size=x.size()[2:])\n",
    "#         if self.pool_C == 'avg':\n",
    "#             x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         output = self.out(x)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetFixed(nn.Module):\n",
    "    def __init__(self, input_chan, kernel, stride, padding, output):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(input_chan, 16, kernel, stride, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, kernel, stride, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(32, 64, kernel, stride, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.out = nn.Linear(64*3*3, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(model):\n",
    "    count=0\n",
    "    for param in list(model.parameters()):\n",
    "        n=1\n",
    "        for s in list(param.size()):\n",
    "            n = n*s\n",
    "        count += n\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chan = 1\n",
    "output = 10\n",
    "kernel = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "model_fixed = NetFixed(input_chan, kernel, stride, padding, output)\n",
    "print('fixed model parameters = ', get_parameters(model_fixed))\n",
    "model_mixed = Net(input_chan, kernel, stride, padding, output, pool='avg')\n",
    "print('mixed model parameters = ', get_parameters(model_mixed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, loader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader['val_set']:\n",
    "            output = net(x)\n",
    "            _,pred_y = torch.max(output, dim = 1)\n",
    "            correct += (pred_y == y).float().sum()\n",
    "\n",
    "    print('accuracy on validation set', (correct / 10000)*100, '%')\n",
    "    return (correct / 10000)*100\n",
    "\n",
    "def loader_loop(net, loader_list, epochs, loss_f, opt):\n",
    "    train_loss = []\n",
    "    epoch_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('epoch = ', i)\n",
    "        for loader in loader_list:\n",
    "            for j, (x, y) in enumerate(loader['train_set']):\n",
    "                opt.zero_grad()\n",
    "                x_batch = x\n",
    "                y_batch = y\n",
    "                output = net.forward(x_batch)\n",
    "                loss = loss_f(output, y_batch)\n",
    "                train_loss.append(loss)\n",
    "                if j % 1000 == 0:\n",
    "                    print('loss:', loss.item())\n",
    "                loss.backward()\n",
    "                opt.step() \n",
    "            validate(net, loader)\n",
    "        epoch_list.append(i)\n",
    "        acc_list.append(validate(net, random.choice(loader_list)))\n",
    "    return [train_loss, epoch_list, acc_list]\n",
    "\n",
    "def image_folder(path):\n",
    "    training_data = torchvision.datasets.ImageFolder(path+'mnist-varres/train', transform =  transforms.Compose([transforms.Grayscale(num_output_channels=1), transforms.ToTensor()]))\n",
    "    test_set= torchvision.datasets.ImageFolder(path+'mnist-varres/test', transform =  transforms.Compose([transforms.Grayscale(num_output_channels=1), transforms.ToTensor()]))\n",
    "    trainsize = round(0.8*len(training_data))\n",
    "    train_set, val_set = torch.utils.data.random_split(training_data, [trainsize, len(training_data)-trainsize]) \n",
    "    print(len(train_set), len(val_set))\n",
    "    return [train_set, val_set, test_set]\n",
    "\n",
    "def get_data(path1, path2, path3):\n",
    "    data1 = image_folder(path1)\n",
    "    data2 = image_folder(path2)\n",
    "    data3 = image_folder(path3)\n",
    "    return data1, data2, data3\n",
    "\n",
    "def get_loaders(batch_size, train_set, val_set, test_set):\n",
    "    loaders = {'train_set' : DataLoader(train_set, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "               'val_set' : DataLoader(val_set, \n",
    "                                          batch_size=len(val_set), \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "                'test_set'  : DataLoader(test_set, \n",
    "                                          batch_size=len(test_set), \n",
    "                                          shuffle=False, \n",
    "                                          num_workers=1)}\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '32x32/'\n",
    "path2 = '48x48/'\n",
    "path3 = '64x64/'\n",
    "data32, data48, data64 = get_data(path1, path2, path3)\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "loaders32 = get_loaders(batch_size, data32[0], data32[1], data32[2])\n",
    "loaders48 = get_loaders(batch_size, data48[0], data48[1], data48[2])\n",
    "loaders64 = get_loaders(batch_size, data64[0], data64[1], data64[2])\n",
    "loader_list = [loaders32, loaders48, loaders64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chan = 1\n",
    "output = 10\n",
    "kernel = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "epochs = 20\n",
    "lr = 0.0001\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "net_avg = Net(input_chan, kernel, stride, padding, output, pool='avg')\n",
    "opt = optim.Adam(net_avg.parameters(), lr)\n",
    "results_avg = loader_loop(net_avg, loader_list, epochs, loss_f, opt)\n",
    "# net_max = Net(input_chan, kernel, stride, padding, output, pool='max')\n",
    "# loss_f = nn.CrossEntropyLoss()\n",
    "# opt = optim.Adam(net_max.parameters(), lr)\n",
    "# results_max = loader_loop(net_max, loader_list, epochs, loss_f, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = results_avg[2]\n",
    "x1 = results_avg[1]\n",
    "plt.plot(x1, y1, label = 'AvgPool')\n",
    "y2 = results_max[2]\n",
    "x2 = results_max[1]\n",
    "plt.plot(x2, y2, label = 'MaxPool')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('figures/q16_N=81')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning on learning rate, batchsize and...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, loaders):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loaders['val_set']:\n",
    "            output = net(x)\n",
    "            _,pred_y = torch.max(output, dim = 1)\n",
    "            correct += (pred_y == y).float().sum()\n",
    "\n",
    "    print('accuracy on validation set', (correct / 10000)*100, '%')\n",
    "    return (correct / 10000)*100\n",
    "\n",
    "def train(net, loaders, epochs, loss_f, opt):\n",
    "    train_loss = []\n",
    "    epoch_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('epoch = ', i)\n",
    "        for j, (x, y) in enumerate(loaders['train_set']):\n",
    "            opt.zero_grad()\n",
    "            x_batch = x\n",
    "            y_batch = y\n",
    "            output = net.forward(x_batch)\n",
    "            loss = loss_f(output, y_batch)\n",
    "            train_loss.append(loss)\n",
    "            if j % 1000 == 0:\n",
    "                print('loss:', loss.item())\n",
    "            loss.backward()\n",
    "            opt.step() \n",
    "        epoch_list.append(i)\n",
    "        acc_list.append(validate(net, loaders))\n",
    "    return train_loss, epoch_list, acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size, train_set, val_set, test_set):\n",
    "    loaders = {'train_set' : DataLoader(train_set, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "               'val_set' : DataLoader(val_set, \n",
    "                                          batch_size=len(val_set), \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "                'test_set'  : DataLoader(test_set, \n",
    "                                          batch_size=len(test_set), \n",
    "                                          shuffle=False, \n",
    "                                          num_workers=1)}\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_size = transforms.Compose(\n",
    "                    [\n",
    "                    transforms.Resize((28,28)),\n",
    "                    transforms.Grayscale(num_output_channels=1),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1308,), (0.3016,)) \n",
    "                    ])\n",
    "\n",
    "training_data = torchvision.datasets.ImageFolder('mnist-varres/train', transform=one_size)\n",
    "test_set= torchvision.datasets.ImageFolder('mnist-varres/test', transform=one_size)\n",
    "train_set, val_set = torch.utils.data.random_split(training_data, [50000, 10000]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chan = 1 \n",
    "output = 10\n",
    "kernel = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "lr = 0.0001\n",
    "net = Net(input_chan, kernel, stride, padding, output, pool='max')\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(net.parameters(), lr)\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "loaders = get_loaders(batch_size, train_set, val_set, test_set) \n",
    "train_loss, epoch_list, acc_list = train(net, loaders, epochs, loss_f, opt)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
